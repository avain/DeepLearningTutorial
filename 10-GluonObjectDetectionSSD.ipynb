{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我們將透過此筆記本來實作SSD物件偵測。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本筆記的目的是完善[MXNet Gluon SSD教學範例](https://zh.gluon.ai/chapter_computer-vision/ssd.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們期望讀者能夠透過這個筆記本來更加地了解SSD演算法。因為這個筆記本是教學範例，我們採用了一個小的合成皮卡丘資料集，並且，使用了一個較小的SSD網路。\n",
    "\n",
    "若想要使用MXNet來訓練SSD，請參考[GluonCV](https://gluon-cv.mxnet.io)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引\n",
    "0. [皮卡丘資料集](#00)\n",
    "1. [測試：```class_predictor(num_anchors, num_classes)```的輸入與輸出](#01)\n",
    "2. [測試：```box_predictor(num_anchors)```的輸入與輸出](#02)\n",
    "3. [測試：```down_sample(num_filters)```的輸入與輸出](#03)\n",
    "4. [測試：```body```的輸入與輸出](#04)\n",
    "5. [定義一個Toy SSD網路](#05)\n",
    "6. [測試：```ToySSD```的輸入與輸出](#06)\n",
    "7. [測試：```iou```的輸入與輸出](#07)\n",
    "8. [測試：```MultiBoxTarget```的輸入與輸出](#08)\n",
    "9. [準備開始訓練 (定義訓練目標, Loss 和Metrics)](#09)\n",
    "10. [訓練模型](#10)\n",
    "11. [模型預測](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon.nn import Sequential\n",
    "from mxnet.contrib.ndarray import MultiBoxPrior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"00\">0. 皮卡丘資料集</a>\n",
    "\n",
    "我們將以皮卡丘資料集來示範如何做物件偵測。首先，我們從該資料集內撈出一個批次的資料，並且，選一張圖，將其畫出來看一看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.image as image\n",
    "\n",
    "data_shape = 256\n",
    "batch_size = 64\n",
    "\n",
    "def get_iterators(data_shape, batch_size):\n",
    "    class_names = ['pikachu']\n",
    "    num_class = len(class_names)\n",
    "    train_iter = image.ImageDetIter(\n",
    "        batch_size=batch_size, \n",
    "        data_shape=(3, data_shape, data_shape),\n",
    "        path_imgrec='../datasets/pikachu/pikachu_train.rec',\n",
    "        path_imgidx='../datasets/pikachu/pikachu_train.idx',\n",
    "        shuffle=True,\n",
    "        mean=True,\n",
    "        rand_crop=1, \n",
    "        min_object_covered=0.95, \n",
    "        max_attempts=200)\n",
    "    val_iter = image.ImageDetIter(\n",
    "        batch_size=batch_size,\n",
    "        data_shape=(3, data_shape, data_shape),\n",
    "        path_imgrec='../datasets/pikachu/pikachu_val.rec',\n",
    "        shuffle=False,\n",
    "        mean=True)\n",
    "    return train_iter, val_iter, class_names, num_class\n",
    "\n",
    "train_data, test_data, class_names, num_class = get_iterators(data_shape, batch_size)\n",
    "batch = train_data.next()         # 取得了一個批次的資料。\n",
    "\n",
    "img = batch.data[0][0].asnumpy()  # grab the first image, convert to numpy array\n",
    "img = img.transpose((1, 2, 0))    # we want channel to be the last dimension\n",
    "img += np.array([123, 117, 104])\n",
    "img = img.astype(np.uint8)        # use uint8 (0-255)\n",
    "\n",
    "# draw bounding boxes on image\n",
    "for label in batch.label[0][0].asnumpy():\n",
    "    if label[0] < 0:\n",
    "        break\n",
    "    xmin, ymin, xmax, ymax = [int(x * data_shape) for x in label[1:5]]\n",
    "    rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, edgecolor=(1, 0, 0), linewidth=3)\n",
    "    plt.gca().add_patch(rect)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,_ in enumerate(train_data):\n",
    "    pass\n",
    "num_batches=idx # see how many batches we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們已經得到了一個批次的資料, 它包含了圖，圖內的物體方框，以及物體方框所屬類別。現在，我們來看一下，一個批次的資料是有什麼樣的形狀："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=batch.data[0]\n",
    "labels=batch.label[0]\n",
    "\n",
    "print(images.shape) # 印出一個批次的Tensor形狀(圖)\n",
    "print(labels.shape) # 印出一個批次的Tensor形狀(標籤)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們來看如何建造SSD網路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{SSD}~\\text{網路架構}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/f1iGeO9.png\" width=\"550\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上圖，可見有四個運算必須實作出來，分別為```Body```, ```down_sample```, ```class_predictor```和```box_predictor```。\n",
    "\n",
    "這四個運算已經放置於```ssd_utils.py```。 現在的問題是，這些運算到底會把輸入變成什麼樣的輸出呢？以下我們將來測試這些運算的輸出，並試著對這些運算有更多的了解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"01\"> 1. 測試： class_predictor(num_anchors, num_classes) 的輸入與輸出</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_utils import class_predictor\n",
    "\n",
    "num_anchors=4 \n",
    "num_classes=5\n",
    "\n",
    "model=class_predictor(num_anchors, num_classes)\n",
    "model.initialize() # 網路需初始化權重方能使用\n",
    "\n",
    "input_tensor =nd.random.normal( shape=(100,3,40,40) )\n",
    "output_tensor = model(input_tensor) # 看input_tensor經過此模型後，會輸出什麼樣的output_tensor\n",
    "print(\"shape of the input tensor=\\t\", input_tensor.shape)\n",
    "print(\"shape of the output tensor=\\t\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```class_predictor``` 是用來預測錨框裡面物體的類別。\n",
    "* 實際上，```class_predictor``` 只是一個convolutional layer。它不會改變原feature map的長寬，而是單純地改變其channel數目。它會將channel數改為 ```num_anchors * (num_classes+1)```。\n",
    "\n",
    "* 由以上範例可見，```input tensor```的每個像素(40X40的其中一個)皆對應了10個預設錨框。而每個預設錨框可預測出```(num_classes+1)```個類別(加上背景也算是一類)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"02\">2. 測試：box_predictor(num_anchors) 的輸入與輸出</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_utils import box_predictor\n",
    "\n",
    "num_anchors=10\n",
    "\n",
    "model=box_predictor(num_anchors)\n",
    "model.initialize() # 網路需初始化權重方能使用\n",
    "\n",
    "input_tensor =nd.random.normal( shape=(100,3,40,40) )\n",
    "output_tensor = model(input_tensor) # 看input_tensor經過此模型後，會輸出什麼樣的output_tensor\n",
    "print(\"shape of the input tensor=\\t\", input_tensor.shape)\n",
    "print(\"shape of the output tensor=\\t\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 實際上，```box_predictor``` 只是一個convolutional layer。它不會改變原feature map的長寬，而是單純地改變其channel數目。它會將輸出的channel數改為 ```num_anchors * 4 ```。也就是說，對於每一個預設錨框，我們希望預測出delta positions(錨框位移)。有了錨框位移之後，我們即可將預設錨框位置加上錨框位移，來微調出物體真正的位置。\n",
    "* 以下是計算錨框位移($d_x,d_y,d_w,d_h$)的方法：\n",
    "\n",
    "    $$    d_x = (g_x - a_x) ~/ a_w$$\n",
    "    $$    d_y = (g_y - a_y) ~/ a_h$$\n",
    "    $$    d_w = \\log( g_w ~/ a_w ) $$\n",
    "    $$    d_h = \\log( g_h ~/ a_h ) $$\n",
    "\n",
    "    以上，$(g_x, g_y,g_w,g_h)$以及$(a_x,a_y,a_w,a_h)$分別是真實方框(ground truth)位置以及預設錨框(anchor)位置。其中，下標的x,y代表了矩形框所在的中心點位置; w,h則代表矩形框的寬和長。\n",
    "\n",
    "    以上，我們將$d_x$, $d_y$分別除以$a_w$或$a_h$的原因是，我們想讓$d_x,d_y$能夠落在差不多的範圍內。\n",
    "    至於，$g_w$或$g_h$為何不個別去減掉$a_w$, $a_h$，而是個別去除以$a_w$, $a_h$，然後取$\\log$? 這是因為，一般來說，預測錨框的長寬和真實方框的長寬有可能差異很大。我們以相除取$\\log$的方式，讓那些比較大的差異減小，這樣可以使機器比較不會去過度注重於調整物體方框的長寬，而忽略了去調整物體方框的中心點位置。\n",
    "\n",
    "    事實上，我們最後還會將$(d_x,d_y,d_w,d_h)$分別拿去除以$(v_x,v_y,v_w,v_h)$ (variance of $x,y,w,h$)，這麼做的目的是讓$d_x,d_y,d_w,d_h$這幾個變量可以再去做進一步的調整。在我們的程式瑪裡，$(v_x,v_y,v_w,v_h)$預設是$(0.1,0.1,0.4,0.4)$。這表示我們將$d_x$, $d_y$放大10倍，$d_w$, $d_h$放大2.5倍。放大的原因是，讓Loss增大。也就是說，希望讓機器除了去學習分類物體以外，也可以去多注意於學習邊框位置的調整。至於為什麼我們把$d_x$, $d_y$放大的比較多呢？那是因為算出來的$d_w$, $d_h$仍然普遍較$d_x$, $d_y$還要大。所以，我們針對$d_w$, $d_h$就不做過多的放大，這樣可以使得$d_x,d_y,d_w,d_h$皆落在差不多的範圍內。\n",
    "    \n",
    "    換句話說，你可以把除以variance這件事情，當作放大$(d_x,d_y,d_w,d_h)$，使得邊框位置偏移的相關Loss可以較大。另一方面，它也協助了我們微調資料$(d_x,d_y,d_w,d_h)$的範圍，讓它們都落在差不多的範圍內，這相當於把資料拿去做標準化(standarization)。一般來說，經過標準化的資料，因為演算法特性的關係，機器所學習出來的結果會比較好。\n",
    "    \n",
    "    最終，我們選擇用以下方式來計算錨框位移：\n",
    "    \n",
    "    $$    d_x = (g_x - a_x) ~/ a_w    ~/ v_x $$\n",
    "    $$    d_y = (g_y - a_y) ~/ a_h    ~/ v_y $$\n",
    "    $$    d_w = \\log( g_w ~/ a_w ) ~/ v_w $$\n",
    "    $$    d_h = \\log( g_h ~/ a_h ) ~/ v_h $$\n",
    "    $$ ~\\\\ $$\n",
    "* $g_x,g_y,g_w,g_h$是透過以下計算得來($a_x,a_y,a_w,a_h$亦同)：\n",
    "\n",
    "    $$ g_x =(g_{x_{min}}+g_{x_{max}})/2$$\n",
    "    $$ g_y =(g_{y_{min}}+g_{y_{max}})/2$$\n",
    "    $$ g_w =g_{x_{max}}-g_{x_{min}}$$\n",
    "    $$ g_h =g_{y_{max}}-g_{y_{min}}$$\n",
    "    \n",
    "    也就是說，矩形框的座標表示方式，可以是$(x,y,w,h)$，也可以是$(x_{min},y_{min},x_{max},y_{max})$。我們可以任意的使用不同的座標表示方法，只要座標轉換是正確的，這些表示方式都代表著同樣的矩形框座標位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在，問題來了，也就是說，我們已經有了人工標記的物件方框位置，但是，我們要怎麼產生出預設錨框位置呢？首先我們先記得一件事情：feature map裡面的每個像素$(i,j)$, 皆對應了數個不同長寬比的預設錨框。我們現在來使用MXNet自帶的```MultiBoxPrior```來產生那些預設錨框："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxPrior\n",
    "\n",
    "# shape: batch x channel x height x weight\n",
    "n = 40\n",
    "x = nd.random.uniform(shape=(1, 3, n, n))\n",
    "\n",
    "y = MultiBoxPrior(x, sizes=[.5,.25,.1], ratios=[1,2,.5]) # 產生數個不同長寬比的錨框\n",
    "\n",
    "boxes = y.reshape((n, n, -1, 4))\n",
    "\n",
    "# 畫出中心點位於像素(i,j)=(20,20)的五個預設錨框\n",
    "import matplotlib.pyplot as plt\n",
    "def box_to_rect(box, color, linewidth=3):\n",
    "    \"\"\"convert an anchor box to a matplotlib rectangle\"\"\"\n",
    "    box = box.asnumpy()\n",
    "    return plt.Rectangle(\n",
    "        (box[0], box[1]), (box[2]-box[0]), (box[3]-box[1]), \n",
    "        fill=False, edgecolor=color, linewidth=linewidth)\n",
    "colors = ['blue', 'green', 'red', 'black', 'magenta']\n",
    "plt.imshow(nd.ones((n, n, 3)).asnumpy())\n",
    "anchors = boxes[20, 20, :, :]\n",
    "for i in range(anchors.shape[0]):\n",
    "    plt.gca().add_patch(box_to_rect(anchors[i,:]*n, colors[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上，假定我們有一個$40\\times 40$的feature map。我們只畫出了中心像素$(i,j)=(20,20)$所對應的預設錨框樣貌。\n",
    "\n",
    "事實上，這個$40\\times 40$的feature map內，每個像素都對應了$5$個不同長寬比的預設錨框。因此，我們總共產生了$40\\times40\\times5=8000$個錨框在這個feature map裡面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"03\">3. 測試：down_sample(num_filters) 的輸入與輸出</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_utils import down_sample\n",
    "\n",
    "num_filters=10\n",
    "\n",
    "model=down_sample(num_filters)\n",
    "model.initialize() # 網路需初始化權重方能使用\n",
    "\n",
    "input_tensor =nd.random.normal( shape=(100,3,40,40) )\n",
    "output_tensor = model(input_tensor) # 看input_tensor經過此模型後，會輸出什麼樣的output_tensor\n",
    "print(\"shape of the input tensor=\\t\", input_tensor.shape)\n",
    "print(\"shape of the output tensor=\\t\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```down_sample``` 是由兩個convolutional layer加上最後的一個Max Pooling layer所組成。通過最後的Max Pooling layer後, 原feature map的長寬會減半。\n",
    "* ```down_sample```的作用：讓我們可以在不同的feature map尺度下，去預測出預設錨框的位置，以及錨框內物件的類別。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"04\">4. 測試：body 的輸入與輸出</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_utils import body\n",
    "\n",
    "model=body()\n",
    "model.initialize()\n",
    "\n",
    "input_tensor =nd.random.normal( shape=(100,3,40,40) )\n",
    "output_tensor = model(input_tensor)\n",
    "print(\"shape of the input tensor=\\t\", input_tensor.shape)\n",
    "print(\"shape of the output tensor=\\t\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 實際上，```body``` 是一個簡單的Conv Neural Network。它裡面內含三個Max Pooling，因此，原feature map的長(寬)會減半三次，其變化為：$40 \\to 20 \\to 10 \\to 5$。 \n",
    "* 我們可由```output_tensor```得知，```body```會將原圖轉化成為64個，每個大小皆為5X5的高階特徵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"05\">5. 定義一個Toy SSD網路</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_prediction(pred):\n",
    "    '''將原本大小為(#samples, #predictions, map_width, map_height)\n",
    "       的張量轉置成(#samples, map_width, map_height, #predictions)之後，\n",
    "       攤平成大小為(#samples, map_width * map_height * #predictions)的二維張量。\n",
    "       \n",
    "       如此一來，我們可發現，於第二個維度上，只要經過#predictions個元素，我們就會跑到下一個pixel的預測結果。\n",
    "       \n",
    "       因為該feature map有map_width * map_height個pixel，故，此二維張量的第二個維度，其長度是\n",
    "       map_width * map_height * #predictions。\n",
    "    '''\n",
    "    return nd.flatten(nd.transpose(pred, axes=(0, 2, 3, 1)))\n",
    "\n",
    "def concat_predictions(preds):\n",
    "    '''將不同尺度下的預測全部合併為一個大的張量。我們之後會拿兩個大的張量去計算Loss，這樣我們計算上會比較有效率。'''\n",
    "    return nd.concat(*preds, dim=1)\n",
    "\n",
    "def toy_ssd_model(num_anchors, num_classes):\n",
    "    \"\"\"回傳一些SSD模組。我們稍後將利用這些模組來組裝成SSD模型。\n",
    "       ---------------------------------------------\n",
    "       輸入\n",
    "         num_anchors: 預設的錨框數\n",
    "         num_classes: 物體類別數\n",
    "       ---------------------------------------------\n",
    "       輸出\n",
    "         body: conv network, 用來得到高階feature maps特徵。\n",
    "         downsamples: 內含3個down sampler, 可將經過body出來的feature maps降採樣三次。\n",
    "         class_preds: 內含4個類別預測器。我們將會拿這四個類別預測器，分別在四個不同的feature maps尺度下，\n",
    "                      預測錨框內物體的類別。\n",
    "         box_preds:   內含4個錨框位置預測器。我們將會拿這四個位置預測器，分別在四個不同的feature maps尺度下，\n",
    "                      預測錨框應有的位置。  \n",
    "    \"\"\"\n",
    "    \n",
    "    downsamples,class_preds,box_preds = [Sequential(),\n",
    "                                         Sequential(),\n",
    "                                         Sequential()]\n",
    "\n",
    "    downsamples.add(*[down_sample(128),\n",
    "                      down_sample(128),\n",
    "                      down_sample(128)])\n",
    "    \n",
    "    for scale in range(4):\n",
    "        class_preds.add(class_predictor(num_anchors, num_classes))\n",
    "        box_preds.add(box_predictor(num_anchors))\n",
    "    \n",
    "    return body(), downsamples, class_preds, box_preds\n",
    "\n",
    "def toy_ssd_forward(x, body, downsamples, class_preds, box_preds, sizes, ratios):     \n",
    "    '''定義Toy SSD模型。'''\n",
    "    # 得到經過body network轉換出來的feature maps。\n",
    "    x = body(x)\n",
    "    \n",
    "    # 圖像經過body network轉換後，會成為feature maps。首先，我們要在這個feature maps的尺度下，產生各種不同大小和比例的\n",
    "    # 預設錨框。接著，我們將預測出錨框位置偏移(這樣我們才知道錨框位置須如何調整才能接近真實方框)和錨框內物體類別。\n",
    "    # 註：若預設錨框內含真實物體，我們才會去預測出錨框位置偏移。\n",
    "    # 預設的錨框位置,預測出來的錨框位置以及錨框內物體類別，將分別存放在\n",
    "    # default_anchors, predicted_boxes 以及 predicted_classes 這三個清單內。\n",
    "    default_anchors =   [ MultiBoxPrior(x, sizes=sizes[0], ratios=ratios[0])\n",
    "                        ]\n",
    "    predicted_boxes =   [ flatten_prediction(box_preds[0](x))\n",
    "                        ]  \n",
    "    predicted_classes = [ flatten_prediction(class_preds[0](x))\n",
    "                        ]\n",
    "\n",
    "    # 接下來就是三次的降採樣(down sampling)。每經過一次降採樣，我們就要在降採樣後的新尺度下去做預測。同樣的，\n",
    "    # 預設的錨框位置,預測出來的錨框位置偏移以及錨框內物體類別，將分別被我們添加至\n",
    "    # default_anchors, predicted_boxes 以及 predicted_classes 這三個清單內。\n",
    "    for i in range(3):\n",
    "        # 降採樣\n",
    "        x = downsamples[i](x)\n",
    "        \n",
    "        # 添加預設錨框位置，預測出來的錨框位置偏移以及錨框內物體類別至各別的清單內。\n",
    "        default_anchors.append( MultiBoxPrior(x, sizes=sizes[i+1], ratios=ratios[i+1])\n",
    "                              )\n",
    "        predicted_boxes.append(flatten_prediction(box_preds[i+1](x))\n",
    "                              )\n",
    "        predicted_classes.append(flatten_prediction(class_preds[i+1](x))\n",
    "                                )\n",
    "    return default_anchors, predicted_classes, predicted_boxes\n",
    "\n",
    "class ToySSD(Block):\n",
    "    '''將Toy SSD模型包裝成一個複雜的layer(在MXNet中，一個layer即為一個\"Block\")。\n",
    "       在這裡面，我們須定義\"forward\"方法。之後，當一個tensor進入此layer時，forward即被觸發。\n",
    "       換句話說，tensor會依照forward所定義的轉換，將原tensor去forward成為新的tensor。\n",
    "    '''\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(ToySSD, self).__init__(**kwargs)\n",
    "        # anchor box sizes for 4 feature scales\n",
    "        self.anchor_sizes = [[.2, .272], [.37, .447], [.54, .619], [.71, .79]]\n",
    "        # anchor box ratios for 4 feature scales\n",
    "        self.anchor_ratios = [[1, 2, .5]] * 4\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.body, self.downsamples, self.class_preds, self.box_preds = toy_ssd_model(4, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        default_anchors, predicted_classes, predicted_boxes = toy_ssd_forward(x, self.body, self.downsamples,\n",
    "            self.class_preds, self.box_preds, self.anchor_sizes, self.anchor_ratios)\n",
    "        # we want to concatenate anchors, class predictions, box predictions from different layers\n",
    "        anchors = concat_predictions(default_anchors)\n",
    "        box_preds = concat_predictions(predicted_boxes)\n",
    "        class_preds = concat_predictions(predicted_classes)\n",
    "        # it is better to have class predictions reshaped for softmax computation\n",
    "        class_preds = nd.reshape(class_preds, shape=(0, -1, self.num_classes + 1))\n",
    "        \n",
    "        return anchors, class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"06\">6. 測試： ToySSD 的輸入與輸出</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "21760/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx=mx.gpu(0)\n",
    "\n",
    "net = ToySSD(1)\n",
    "net.initialize(ctx=ctx)\n",
    "x = nd.zeros((10, 3, 256, 256),ctx=ctx)\n",
    "default_anchors, class_predictions, box_predictions = net(x)\n",
    "print('1. shape of the default anchor boxes=\\t', default_anchors.shape)\n",
    "print('2. shape of class predictions=\\t\\t'     , class_predictions.shape)\n",
    "print('3. shape of box predictions=\\t\\t'       , box_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ```default_anchors``` 存的是所有錨框的相應位置$(x_{min},y_{min},x_{max},y_{max})$。我們總共有5440個錨框。\n",
    "2. ```class_predictions```存放的是每張圖，每個錨框，其內含物體的預測類別(1類+背景=2類)。\n",
    "3. ```box prediction```存放的是每張圖，每個錨框，其預測的錨框位置偏移。\n",
    "\n",
    "註： 之後softmax會將```class_predictions```的最後一個維度內資訊轉換成機率。我們之後即可知道，於不同的圖 & 不同的預設錨框內，其各種物體的預測機率為何。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們現在已經差不多定義好網路架構了，但是，我們還是不知道如何算預設方框和真實方框的重疊程度。在這裡，我們將利用IoU(Intersection over Union)來計算兩方框間的重疊程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"07\">7. 測試：iou 的輸入與輸出</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_utils import iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_a=nd.array([[0.,0.,1.,1.],]) # 由box_a的(x_min,y_min,x_max,y_max)我們即可決定出box_a位置\n",
    "box_b=nd.array([[0.,0.,1.,1.],]) # 由box_b的(x_min,y_min,x_max,y_max)我們即可決定出box_b位置\n",
    "print(iou(box_a,box_b))          # 印出box_a與box_b的IoU (應為1*1/1 = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上範例，兩方框位置和大小相等，IoU為1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_a=nd.array([[0.,0.,1.,1.],])   # 由box_a的(x_min,y_min,x_max,y_max)我們即可決定出box_a位置\n",
    "box_b=nd.array([[0.,0.,0.5,0.5],]) # 由box_b的(x_min,y_min,x_max,y_max)我們即可決定出box_b位置\n",
    "print(iou(box_a,box_b))            # 印出box_a與box_b的IoU (應為0.5*0.5/1 = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上範例，兩方框交集為0.25, 聯集為1, 故IoU(交集/聯集)=0.25/1=0.25。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_a=nd.array([[0.,0.,1.,1.],]) # 由box_a的(x_min,y_min,x_max,y_max)我們即可決定出box_a位置\n",
    "box_b=nd.array([[2.,2.,3.,3.],]) # 由box_b的(x_min,y_min,x_max,y_max)我們即可決定出box_b位置\n",
    "print(iou(box_a,box_b))          # 印出box_a與box_b的IoU (應為0/2 = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上範例，兩方框交集為0, 聯集為2, 故IoU(交集/聯集)=0/2=0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"08\">8. 測試：MultiBoxTarget 的輸入與輸出</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們接著來介紹```ssd_utils```內的```MultiBoxTarget```，它可以將真實方框(ground truth boxes)和預設錨框(default anchor boxes)做配對。原則上，只要真實方框和預設錨框的 IoU (Intersection over Union)$~>0.5$, 我們就會將其配對。 除此之外，我們也確保了，於單張圖內，每一個真實方框皆有配對到一個或以上的預設錨框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_utils import MultiBoxTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx=mx.gpu(0)\n",
    "\n",
    "net = ToySSD(1)\n",
    "net.initialize(ctx=ctx)\n",
    "num_figs=2# 只取2張皮卡丘的圖\n",
    "\n",
    "# 得到預設錨框位置，錨框內物體類別預測以及錨框位置預測\n",
    "default_anchors, class_predictions, box_predictions = net(images[:num_figs].as_in_context(ctx))\n",
    "\n",
    "# 得到真實錨框和預設錨框差距，錨框掩蓋碼，錨框類別，錨框類別掩蓋碼\n",
    "anchor_shifts,box_mask,anchor_classes,classes_mask = MultiBoxTarget(default_anchors,\n",
    "                                                                    class_predictions,\n",
    "                                                                    labels[:num_figs].as_in_context(ctx),\n",
    "                                                                    hard_neg_ratio=3,\n",
    "                                                                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為設定了```verbose=True```，所以這個方法有輸出一些診斷訊息。 訊息當中，比較需要被關注的，是針對不同的輸入圖所印出來的一張表。該表告訴我們，在該輸入圖當中，不同的物體類別，分別對應了多少個預設錨框。\n",
    "\n",
    "以上，我們的物體類別分別是-1(無皮卡丘),0(無皮卡丘),1(有皮卡丘)。我們可以發現，0類錨框數是1類錨框數的3倍，那是因為我們設定了```hard_neg_ratio=3```。 一般來說， 因為背景樣本相當的多，所以機器在學的時候，有可能會去過於關注於背景樣本的學習。換句話說，過多的負樣本容易導致機器不太知道該如何來分類正樣本(含有物體的樣本)。因此，在這裡我們使用了所謂的hard negative mining法。hard negative mining是指，我們先將負類樣本特別去採樣出比較難學的樣本(hard negative)，然後將其放置於0這一類。至於其他的負類錨框，我則將其放置於-1這一類。之後，在我們分類的時候，我們將捨棄-1這類的樣本，因為這類的樣本比較好學。\n",
    "\n",
    "* Q: 何謂hard negative樣本? A: 直覺的想，一看就知道是背景的圖，機器會認定它比較好學。而倘若有背景和真實物體夾雜的情況，該負樣本就很有可能是對於機器來說，比較難學的樣本。\n",
    "\n",
    "* Q: 如何採樣出hard negative樣本？ A: 我們只要去讓機器預測出該樣本是負類的機率即可。要是機器非常不確定該樣本是負類，那麼該樣本就會是hard negative樣本。 實際上我們的作法是去將負樣本的信心度做排序，讓機器把它覺得最沒信心是負樣本的那些樣本，去當作hard negative樣本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著我們來研究一下```MultiBoxTarget```所輸出的Tensors的形狀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of anchor_shifts=\\t\\t\",anchor_shifts.shape)\n",
    "print(\"Shape of box_mask=\\t\\t\",box_mask.shape)        \n",
    "print(\"Shape of anchor_classes=\\t\",anchor_classes.shape)  \n",
    "print(\"Shape of classes_mask=\\t\\t\",classes_mask.shape)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```anchor_shifts```儲存了真實物體方框和預設錨框的位置差距。只有當預設錨框內有物體，才會去做這個計算。\n",
    "* ```box_mask```是用來遮蓋掉沒有配對到真實物體的那些預設錨框。那些沒有配對到真實物體的預設錨框並沒有被計算出其和真實方框的位置差距。\n",
    "* ```anchor_classes```儲存了每個錨框的類別(可能是-1/0/1)。\n",
    "* ```classes_mask```是用來遮蓋掉不需要被採納的負類別(類別是-1的樣本過多，將被我們捨棄)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"09\">9. 準備開始訓練 (定義訓練目標, Loss 和Metrics)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_targets(default_anchors, class_predicts, labels):\n",
    "    '''定義訓練目標。\n",
    "       目標：1. 學習真實方框和預設錨框的位置差距。這樣我們就可以知道怎麼微調預設錨框位置。\n",
    "               另外，我們需要掩蓋碼，它可以幫我們遮蓋掉沒有配對到真實物體的那些預設錨框。\n",
    "               這些資訊將分別存在box_target和box_mask之中。\n",
    "            2. 學習分類物體。\n",
    "               另外，我們需要掩蓋碼，它可以幫我們遮蓋掉大量的，容易學習的負類錨框。\n",
    "               這些資訊將分別存在cls_target和classes_mask之中。\n",
    "    '''\n",
    "    z = MultiBoxTarget(*[default_anchors,class_predicts ,labels],\n",
    "                       hard_neg_ratio=4)\n",
    "    box_target = z[0]   # 真實物體方框和預設錨框的位置差距\n",
    "    box_mask = z[1]     # 用來遮蓋掉沒有配對到真實物體的那些預設錨框\n",
    "    cls_target = z[2]   # 儲存了每個錨框的類別\n",
    "    classes_mask = z[3] # 用來遮蓋掉不需要被採納的負類別\n",
    "    return box_target, box_mask, cls_target,classes_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(gluon.loss.Loss):\n",
    "    '''定義Focal Loss。它是用來計算真實物體類別和預測物體類別差異。\n",
    "       這個Loss的好處是，較易學習的樣本，其Loss會較小。較難學習的樣本，其Loss會較大。\n",
    "       \n",
    "       如此，機器可以聚焦在比較難學習的樣本。\n",
    "       \n",
    "       Q: 什麼是難學習/易學習的樣本？ A: 若某樣本是正類，但機器認為它有很高的機會是負類，則該樣本會是難學習的樣本。\n",
    "    '''\n",
    "    def __init__(self, axis=-1, alpha=0.25, gamma=2, batch_axis=0, **kwargs):\n",
    "        super(FocalLoss, self).__init__(None, batch_axis, **kwargs)\n",
    "        self._axis = axis\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label, classes_mask):\n",
    "        output = F.softmax(output)\n",
    "        pt = F.pick(output, label, axis=self._axis, keepdims=False)\n",
    "        loss = classes_mask * -self._alpha * ((1 - pt) ** self._gamma) * F.log(pt)\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
    "\n",
    "# cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "cls_loss = FocalLoss()\n",
    "print(cls_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(gluon.loss.Loss):\n",
    "    '''我們會利用此Loss去學習預測方框應該偏移多少才會接近真實方框。\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, batch_axis=0, **kwargs):\n",
    "        super(SmoothL1Loss, self).__init__(None, batch_axis, **kwargs)\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label, mask):\n",
    "        loss = F.smooth_l1((output - label) * mask, scalar=1.0)\n",
    "        return F.mean(loss, self._batch_axis, exclude=True)\n",
    "\n",
    "box_loss = SmoothL1Loss()\n",
    "print(box_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_metric = mx.metric.F1()   # 因為負類樣本多，會導致accuracy相當好。如此，我們由accuracy就不容易看出分類的好壞。\n",
    "                              # 另一方面，F1是綜合precision和recall的一個複合指標，使用它的好處是，它不會過於關注\n",
    "                              # 負類樣本分類的accuracy，當負類樣本多時，這個指標是比較恰當的評估模型方式。\n",
    "                              # 因此我們這裡將採用F1來評估模型。\n",
    "box_metric = mx.metric.MAE()  # 我們用mean absolute error來評量方框偏差估計的好壞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"10\">10. 訓練模型</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(0) # 使用GPU0\n",
    "\n",
    "net = ToySSD(num_class)\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15           # set larger to get better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我們將利用trainer來更新網路權重。\n",
    "trainer = gluon.Trainer(net.collect_params(),'nadam',{'wd': 1.E-4}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from mxnet import autograd as ag\n",
    "\n",
    "models=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # reset iterator and tick\n",
    "    train_data.reset()\n",
    "    cls_metric.reset()\n",
    "    box_metric.reset()\n",
    "    tic = time.time()\n",
    "    # iterate through all the batches (except for the last one)\n",
    "    for i in range(num_batches-1):\n",
    "        batch=train_data.next()\n",
    "        btic = time.time()\n",
    "        # record gradients\n",
    "        with ag.record():\n",
    "            x = batch.data[0].as_in_context(ctx)\n",
    "            y = batch.label[0].as_in_context(ctx)\n",
    "            default_anchors, class_predictions, box_predictions = net(x)\n",
    "\n",
    "            with ag.pause():\n",
    "                box_target, box_mask, cls_target,classes_mask = training_targets(default_anchors, class_predictions, y)\n",
    "            # losses\n",
    "            loss1 = cls_loss(class_predictions, cls_target, classes_mask)\n",
    "            loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "            # sum all losses\n",
    "            loss = loss1 + loss2\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "        # apply \n",
    "        trainer.step(batch_size)\n",
    "        # update metrics\n",
    "        cls_metric.update([cls_target.clip(0,np.nan)], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        #cls_metric.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric.update([box_target], [box_predictions * box_mask])\n",
    "    \n",
    "    # end of epoch logging\n",
    "    name1, val1 = cls_metric.get()\n",
    "    name2, val2 = box_metric.get()\n",
    "    \n",
    "    # saving model at the last five epochs\n",
    "    if epoch in range(epochs-5,epochs):\n",
    "        models.append((val1,val2,net))\n",
    "\n",
    "    print('[Epoch %d] training: %s=%f, %s=%f, time elapsed=%f'%(epoch, name1, val1, name2, val2, time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# retrieve the saved models, turning it to a Pandas table\n",
    "df=pd.DataFrame(models,columns=[\"f1\",\"mae\",\"model\"])\n",
    "df=df.sort_values([\"f1\"],ascending=False) # sort model according to its performance\n",
    "df.head(3) # show the top three best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best model for prediction\n",
    "best_model=df.iloc[0][\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"11\">11. 模型預測</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "def preprocess(image):\n",
    "    \"\"\"Takes an image and apply preprocess\"\"\"\n",
    "    # resize to data_shape\n",
    "    image = cv2.resize(image, (data_shape, data_shape))\n",
    "    # swap BGR to RGB\n",
    "    image = image[:, :, (2, 1, 0)]\n",
    "    # convert to float before subtracting mean\n",
    "    image = image.astype(np.float32)\n",
    "    # subtract mean\n",
    "    image -= np.array([123, 117, 104])\n",
    "    # organize as [batch-channel-height-width]\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    image = image[np.newaxis, :]\n",
    "    # convert to ndarray\n",
    "    image = nd.array(image)\n",
    "    return image\n",
    "\n",
    "image = cv2.imread('../datasets/pikachu/pikachu.jpg')\n",
    "x = preprocess(image)\n",
    "print('shape of x=', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, cls_preds, box_preds = best_model(x.as_in_context(ctx))\n",
    "print('anchors', anchors)\n",
    "print('class predictions', cls_preds)\n",
    "print('box delta predictions', box_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後，我們將借助MXNet內建的```MultiBoxDetection```，把模型的預測結果顯現出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxDetection\n",
    "# convert predictions to probabilities using softmax\n",
    "cls_probs = nd.SoftmaxActivation(nd.transpose(cls_preds, (0, 2, 1)), mode='channel')\n",
    "# apply shifts to anchors boxes, non-maximum-suppression, etc...\n",
    "output = MultiBoxDetection(*[cls_probs, box_preds, anchors],\n",
    "                           force_suppress=True,\n",
    "                           clip=False,\n",
    "                           variances=(0.1,0.1,0.4,0.4)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img, out, thresh=0.5):\n",
    "    \n",
    "    import random\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams['figure.figsize'] = (10,10)\n",
    "    pens = dict()\n",
    "    plt.clf()\n",
    "    plt.imshow(img)\n",
    "    for det in out:\n",
    "        cid = int(det[0])\n",
    "        if cid < 0:\n",
    "            continue\n",
    "        score = det[1]\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        if cid not in pens:\n",
    "            pens[cid] = (random.random(), random.random(), random.random())\n",
    "        scales = [img.shape[1], img.shape[0]] * 2\n",
    "        xmin, ymin, xmax, ymax = [int(p * s) for p, s in zip(det[2:6].tolist(), scales)]\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, \n",
    "                             edgecolor=pens[cid], linewidth=3)\n",
    "        plt.gca().add_patch(rect)\n",
    "        text = class_names[cid]\n",
    "        plt.gca().text(xmin, ymin-2, '{:s} {:.3f}'.format(text, score),\n",
    "                       bbox=dict(facecolor=pens[cid], alpha=0.5),\n",
    "                       fontsize=12, color='white')\n",
    "    plt.show()\n",
    "    \n",
    "display(image[:, :, (2, 1, 0)], output[0].asnumpy(), thresh=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上，我們設定了```thresh=0.4```，也就是說，置性度高於0.4的預測方框，我們才會將其顯現出來。你可以試著增加或降低這個值，來看皮卡丘的預測會不會受到影響。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
