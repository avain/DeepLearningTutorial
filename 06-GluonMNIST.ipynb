{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本筆記將帶大家熟悉MXNet Gluon\n",
    "# 此範例使用Fasion MNIST資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 索引\n",
    "\n",
    "1. [選擇Device為GPU0](#1.-選擇Device為GPU0)\n",
    "2. [小練習：向量相加](#2.-小練習：向量相加)\n",
    "3. [準備資料: Fashion MNIST dataset](#3.-準備資料:-Fashion-MNIST-dataset)\n",
    "4. [資料視覺化](#4.-資料視覺化)\n",
    "5. [建立 mini-batch data generator](#5.-建立-mini-batch-data-generator)\n",
    "6. [定義模型](#6.-定義模型)\n",
    "7. [初始化模型權重參數](#7.-初始化模型權重參數)\n",
    "8. [定義Loss](#8.-定義Loss)\n",
    "9. [定義Optimizer](#9.-定義Optimizer)\n",
    "10. [定義Trainer](#10.-定義Trainer)\n",
    "10. [訓練一個imperative模型](#11.-訓練一個imperative模型)\n",
    "10. [複合模型與混精度加速](#12.-複合模型與混精度加速)\n",
    "10. [建立&訓練一個沒有Dense層的CNN網路](#13.-建立&訓練一個沒有Dense層的CNN網路)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon,autograd\n",
    "import numpy as np\n",
    "\n",
    "import gzip\n",
    "from os.path import join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 選擇Device為GPU0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(0) # we'll \"flow our tensors\" with GPU0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 小練習：向量相加\n",
    "\n",
    "於imperative模式, 我們可使用GPU做向量相加，非常容易。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=nd.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2=nd.array([1,2,3],ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3=arr.as_in_context(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2+arr3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 準備資料: Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(data,label):\n",
    "    return (nd.transpose(data,axes=(2,0,1) ).astype(np.float32) / 255.),label\n",
    "\n",
    "# 載入資料，並將圖的值除以255，使其範圍落於[0,1]的區間內。\n",
    "data_train=[d for d in gluon.data.vision.FashionMNIST(root=\"../datasets/fashion_mnist/\", train=True,transform=trans)]\n",
    "data_test=[d for d in gluon.data.vision.FashionMNIST(root=\"../datasets/fashion_mnist/\", train=False,transform=trans)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 資料視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(2,5,figsize=(7,3))\n",
    "\n",
    "for row_axes in axes:\n",
    "    for col_axis in row_axes:\n",
    "        # randomly choose a figure to plot\n",
    "        rand_num=np.random.choice(len(data_train))\n",
    "        idx=rand_num\n",
    "        image=data_train[idx][0][0]\n",
    "        label_image=data_train[idx][1]\n",
    "        # plot the chosen figure\n",
    "        col_axis.imshow((image.asnumpy()*255.).astype(np.float32),cmap=\"gray\")\n",
    "        col_axis.axis(\"off\")\n",
    "        col_axis.set_title(label_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of train data=\\t\",len(data_train))\n",
    "print(\"number of test data=\\t\",len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 建立 mini-batch data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen=gluon.data.DataLoader(data_train,batch_size=128)\n",
    "test_gen=gluon.data.DataLoader(data_test,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= test block =======\n",
    "# let's double check if the mini-batch generator works as expected\n",
    "for batch_images,batch_labels in test_gen:\n",
    "    print(batch_images.shape,batch_labels.shape)\n",
    "    break\n",
    "# ======= test block ends ======="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定義模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就如同Keras一樣，於模型建立階段，MXNet Gluon亦可以使用 *Sequential* 的方式逐一疊加網路層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# exercise: let's build a MLP\n",
    "# ====================================\n",
    "# model=nn.Sequential()\n",
    "# with model.name_scope():\n",
    "#     model.add( nn.Flatten())\n",
    "#     model.add( ...\n",
    "#              )\n",
    "#     model.add( ...\n",
    "#              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following is a simple conv neural network\n",
    "model=nn.Sequential()\n",
    "\n",
    "with model.name_scope():\n",
    "    model.add(nn.Conv2D(64, kernel_size=3, strides=1,padding=1, activation=\"relu\"))\n",
    "    model.add(nn.Conv2D(64, kernel_size=2, strides=1, activation=\"relu\") )\n",
    "    model.add(nn.MaxPool2D())\n",
    "    model.add(nn.Conv2D(128, kernel_size=2, strides=1, activation=\"relu\") )\n",
    "    model.add(nn.Conv2D(128, kernel_size=2, strides=1, activation=\"relu\") )\n",
    "    model.add(nn.MaxPool2D())\n",
    "    model.add(nn.Flatten())\n",
    "    model.add(nn.Dense(128, activation=\"relu\"))\n",
    "    model.add(nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 初始化模型權重參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.collect_params().initialize(mx.init.Xavier(),ctx=ctx) # 權重參數初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== TEST BLOCK BEGINS ======================================\n",
    "# Let's check the forward pass, by simply feeding a batch of random images into \n",
    "# the model we have just built.\n",
    "images=np.random.normal(0,1,(32,1,28,28))\n",
    "images=nd.array(images,ctx=ctx,dtype=np.float32)\n",
    "print(model(images).shape)\n",
    "# ===================== TEST BLOCK ENDS ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= TEST BLOCK BEGINS =================================================\n",
    "nd.softmax(model(images)).sum(axis=1) # let's check if the probabilities are conserved if \n",
    "                                      # we feed the output to a softmax layer\n",
    "# ========================= TEST BLOCK ENDS ==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 定義Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxCrossEntropyLoss=gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 定義Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=mx.optimizer.SGD(momentum=0.9,\n",
    "                     wd=0.001,\n",
    "                     learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 定義Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trainer helps us to fetch all the parameters & gradients and update them with the help of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=gluon.Trainer(params=model.collect_params(),optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 訓練一個imperative模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muon import CreateModel # 給原model添加fit等功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 我們利用CreateModel來賦予model新的功能，也就是：fit。\n",
    "model=CreateModel(model,ctx=ctx,precision=np.float32)\n",
    "\n",
    "# 初始化模型權重參數\n",
    "model.collect_params().initialize( mx.init.Xavier(),\n",
    "                                   ctx=model.ctx,\n",
    "                                   force_reinit=True)\n",
    "\n",
    "# 使用 model.fit 訓練模型\n",
    "model.fit(train_gen,\n",
    "          test_gen,\n",
    "          epochs=3,\n",
    "          print_every=200,\n",
    "          loss_with_softmax=softmaxCrossEntropyLoss,\n",
    "          optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 複合模型與混精度加速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 複合模型與混精度加速\n",
    "* 若模型是複合模型，利用```model.hybridize()```可將模型由原本的imperative模式切換至symbolic模式。若資料量足夠，一般來說，此切換可將模型訓練速度提升至約2x。\n",
    "* 當資料與模型權重參數皆使用半精度做儲存時，我們可設定Optimizer是```multi_precision=True```。如此一來，我們即可利用*單半混合精度* (mixed precision)的方式來訓練模型。\n",
    "\n",
    "#### 補充\n",
    "1. NVIDIA新的GPU架構(Volta)支持單半混精加速。\n",
    "2. 單 / 半精度亦即數值使用FP32(4個bytes) / FP16(2個bytes)來儲存。\n",
    "\n",
    "#### 參考資料\n",
    "* mixed precision training\n",
    "    * [NVIDIA's SDK](http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html#training_mxnet)\n",
    "    * [Info from NVIDIA's blog](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/)\n",
    "    * [Paper](https://arxiv.org/pdf/1710.03740.pdf)\n",
    "* hybridizing (MXNet Gluon)\n",
    "    * [命令式和符號式混合编程](https://zh.gluon.ai/chapter_computational-performance/hybridize.html#命令式和符号式混合编程)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muon import CreateHybridModel # 給複合模型添加fit等功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 建立複合(hybrid)模型\n",
    "layer=nn.HybridSequential()\n",
    "with layer.name_scope():\n",
    "    layer.add(nn.Conv2D(64, kernel_size=3, strides=1,padding=1, activation=\"relu\"))\n",
    "    layer.add(nn.Conv2D(64, kernel_size=2, strides=1, activation=\"relu\") )\n",
    "    layer.add(nn.MaxPool2D())\n",
    "    layer.add(nn.Conv2D(128, kernel_size=2, strides=1, activation=\"relu\") )\n",
    "    layer.add(nn.Conv2D(128, kernel_size=2, strides=1, activation=\"relu\") )\n",
    "    layer.add(nn.MaxPool2D())\n",
    "    layer.add(nn.Flatten())\n",
    "    layer.add(nn.Dense(128, activation=\"relu\"))\n",
    "    layer.add(nn.Dense(10))\n",
    "\n",
    "model=CreateHybridModel(layer,ctx=ctx,precision=np.float16)\n",
    "\n",
    "# 初始化模型權重參數\n",
    "model.collect_params().initialize(mx.init.Xavier(),ctx=model.ctx)\n",
    "\n",
    "# 將模型切換至Symbolic模式\n",
    "model.hybridize()\n",
    "\n",
    "# 定義Loss和Optimizer\n",
    "softmaxCrossEntropyLoss=gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "opt=mx.optimizer.Adam(multi_precision=True)\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(train_gen,\n",
    "          test_gen,\n",
    "          epochs=3,\n",
    "          print_every=200,\n",
    "          loss_with_softmax=softmaxCrossEntropyLoss,\n",
    "          optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 建立&訓練一個沒有Dense層的CNN網路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "制定模組```conv_pooling_block```，這樣以後建立網路時，可以重複利用該模組。\n",
    "\n",
    "除此之外，Conv層之間添加BatchNorm，可使得訓練比較穩定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_pooling_block(num_filters, kernel_size=3, padding=1):\n",
    "    '''A basic building block.'''\n",
    "\n",
    "    layer=nn.HybridSequential()\n",
    "    if num_filters!=None:\n",
    "        with layer.name_scope():        \n",
    "            layer.add(nn.Conv2D(num_filters, kernel_size=kernel_size, padding=padding))\n",
    "            layer.add(nn.Activation(\"relu\"))\n",
    "            layer.add(nn.BatchNorm())\n",
    "    else:\n",
    "        with layer.name_scope():        \n",
    "            layer.add(nn.MaxPool2D())\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 模型設定\n",
    "num_filters=[64, 64, None, 128, 128, None] \n",
    "config_clf_layers=[(512,5,2),(512,7,3)]\n",
    "\n",
    "# 根據模型設定，建立複合模型\n",
    "layer=nn.HybridSequential()\n",
    "with layer.name_scope():\n",
    "    layer.add( *[conv_pooling_block(num_filter) for num_filter in num_filters] )    # feature extraction part\n",
    "    layer.add( *[conv_pooling_block(*config) for config in config_clf_layers]  )    # classification part\n",
    "    layer.add( nn.Conv2D(10,kernel_size=1) )\n",
    "    layer.add( nn.Flatten() )\n",
    "\n",
    "model=CreateHybridModel(layer,ctx=ctx,precision=np.float16)\n",
    "\n",
    "# 初始化模型權重參數\n",
    "model.collect_params().initialize(mx.init.Xavier(),ctx=model.ctx)\n",
    "model.hybridize()\n",
    "\n",
    "# 定義Loss和Optimizer\n",
    "softmaxCrossEntropyLoss=gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "opt=mx.optimizer.Adam(multi_precision=True)\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(train_gen,\n",
    "          test_gen,\n",
    "          epochs=3,\n",
    "          print_every=200,\n",
    "          loss_with_softmax=softmaxCrossEntropyLoss,\n",
    "          optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[返回索引]](#索引)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
